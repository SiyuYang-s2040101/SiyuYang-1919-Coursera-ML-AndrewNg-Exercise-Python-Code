{
 "cells": [
  {
   "source": [
    "# Exercise Description\n",
    "## Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university.\n",
    "Suppose that you are the administrator of a university department and\n",
    "you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\n",
    "- Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "## 1. Extract the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Github\n",
    "url = 'https://raw.githubusercontent.com/SiyuYang-1919/Coursera-ML-AndrewNg-Notes/master/code/ex2-logistic%20regression/ex2data1.txt'\n",
    "data = pd.read_csv(url, delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adm = data.rename(columns={0: 'Exam_1', 1: 'Exam_2', 2: 'Admission'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adm.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert theta_0 (=1)\n",
    "Adm.insert(0, 'ones', 1)"
   ]
  },
  {
   "source": [
    "## 2. Visualizing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = Adm.loc[Adm['Admission'] == 1]\n",
    "neg = Adm.loc[Adm['Admission'] == 0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=pos['Exam_1'], y=pos['Exam_2'], c='dodgerblue', marker='+', label='Admitted')\n",
    "ax.scatter(x=neg['Exam_1'], y=neg['Exam_2'], c='r', marker='o', label='Not Admitted')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')\n",
    "ax.set_title('Figure 1: Scatter plot of training data')\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "## 3. Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 3.1 Sigmoid function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sigmoid function\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "    This function is to calculate the corresponding value of the input value through the sigmoid function.\n",
    "\n",
    "    Input:\n",
    "         x(ndarray): the input value\n",
    "    \n",
    "    Output:\n",
    "         g(ndarray): the correspondent value calculated through the sigmoid function.\n",
    "    '''\n",
    "    g = 1 / (1 + np.exp(-x))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "print(sigmoid(0))\n",
    "print(sigmoid(10))\n",
    "print(sigmoid(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0, 10, -10])\n",
    "sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(10).reshape(5, 2)\n",
    "sigmoid(b)"
   ]
  },
  {
   "source": [
    "### 3.2 Cost function and gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cost function for logistic regression\n",
    "def costfunction(theta, m, X, y):\n",
    "    '''\n",
    "    This function is to calculate the cost of a logistic regression model.\n",
    "\n",
    "    Input:\n",
    "         theta(vector with shape (3, )): parameters\n",
    "         m(int,the number of rows in X or y, =X.shape[0]): the number of data groups\n",
    "         X(2-D array): training data\n",
    "         y(2-D array): flags\n",
    "    Output:\n",
    "         cost(float): the cost of a logistic regression model\n",
    "    '''\n",
    "    theta = theta.reshape(1, 3)\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    inner = - y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))\n",
    "    cost = sum(inner) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "m_1 = Adm.shape[0]\n",
    "theta_1 = np.array([0, 0, 0])\n",
    "X_1 = Adm.drop(['Admission'], axis=1)\n",
    "y_1 = Adm['Admission']\n",
    "costfunction(theta_1, m_1, X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to perform gradient descent\n",
    "def gradient(theta, m, X, y):\n",
    "    '''\n",
    "    This function is to calculate the gradient of cost function.\n",
    "\n",
    "    Input:\n",
    "         theta(vector with shape (3, )): parameters\n",
    "         m(int,the number of rows in X or y, =X.shape[0]): the number of data groups\n",
    "         X(2-D array): training data\n",
    "         y(2-D array): flags\n",
    "    Output:\n",
    "         gd.flatten()(ndarray, vector with shape (3, )): gradients of each variable\n",
    "    '''\n",
    "    diff = sigmoid(X.dot(theta.T)) - y\n",
    "    gd = np.dot(X.T, diff) / m\n",
    "    return gd.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(theta_1, m_1, X_1, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use minimize() to get the values of theta that minimize the cost function\n",
    "# Similar to fminunc in Octave\n",
    "import scipy.optimize as sop\n",
    "theta_initial = np.array([0, 0, 0])\n",
    "result = sop.minimize(fun=costfunction, x0=theta_initial, args=(m_1, X_1, y_1), method='TNC', jac=gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_op = result.x"
   ]
  },
  {
   "source": [
    "### 3.3 Decision boundary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=pos['Exam_1'], y=pos['Exam_2'], c='dodgerblue', marker='+', label='Admitted')\n",
    "ax.scatter(x=neg['Exam_1'], y=neg['Exam_2'], c='r', marker='o', label='Not Admitted')\n",
    "x_1 = np.arange(30, 98)\n",
    "x_2 = (- theta_op[0] - theta_op[1]*x_1) / theta_op[2]\n",
    "ax.plot(x_1, x_2, 'b-')\n",
    "ax.set_xlabel('Exam 1 score')\n",
    "ax.set_ylabel('Exam 2 score')\n",
    "ax.set_title('Figure 2: Training data with decision boundary')\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "### 3.4 Evaluating logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Write a function to make predictions\n",
    "def prediction(theta, X, type):\n",
    "    '''\n",
    "    This function is to make predictions with new data.\n",
    "\n",
    "    Input:\n",
    "         theta(vector with shape (3, )): parameters that minimize the cost function\n",
    "         X(2-D array): training data\n",
    "         type(str): 'accepted'-predictions about whether being accepted\n",
    "                    'both'-predictions about the probability of being accepted and whether being accepted\n",
    "                    other-predictions about the probability of being accepted\n",
    "    Output:\n",
    "          probability(ndarray): the probability of being accepted\n",
    "          accepted(ndarray): whether being accepted\n",
    "    '''\n",
    "    probability = sigmoid(X.dot(theta.T))\n",
    "    if type == 'accepted':\n",
    "        probability[probability >= 0.5] = 1\n",
    "        probability[probability < 0.5] = 0\n",
    "        return probability\n",
    "    elif type == 'both':\n",
    "        accepted = copy.deepcopy(probability)\n",
    "        accepted[accepted >= 0.5] = 1\n",
    "        accepted[accepted < 0.5] = 0\n",
    "        return probability, accepted\n",
    "    else:\n",
    "        return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test = np.array([[1, 45, 85]])\n",
    "prediction(theta_op, X_test, 'both')"
   ]
  },
  {
   "source": [
    "## Regularized logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- In this part of the exercise, you will implement regularized logistic regression\n",
    "to predict whether microchips from a fabrication plant passes quality assurance\n",
    "(QA). During QA, each microchip goes through various tests to ensure\n",
    "it is functioning correctly.\n",
    "- Suppose you are the product manager of the factory and you have the\n",
    "test results for some microchips on two different tests. From these two tests,\n",
    "you would like to determine whether the microchips should be accepted or\n",
    "rejected. To help you make the decision, you have a dataset of test results\n",
    "on past microchips, from which you can build a logistic regression model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Extract the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'https://raw.githubusercontent.com/SiyuYang-1919/Coursera-ML-AndrewNg-Notes/master/code/ex2-logistic%20regression/ex2data2.txt'\n",
    "data1 = pd.read_csv(url1, delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip = data1.rename(columns={0: 'Test1', 1: 'Test2', 2: 'Accepted'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "microchip.describe()"
   ]
  },
  {
   "source": [
    "## 2. Visualizing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = microchip.loc[microchip['Accepted'] == 1]\n",
    "rej = microchip.loc[microchip['Accepted'] == 0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x=acc['Test1'], y=acc['Test2'], c='dodgerblue', marker='+', label='Accepted')\n",
    "ax.scatter(x=rej['Test1'], y=rej['Test2'], c='r', marker='o', label='Rejected')\n",
    "ax.set_xlabel('Test 1 score')\n",
    "ax.set_ylabel('Test 2 score')\n",
    "ax.set_title('Figure 3: Scatter plot of training data')\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "- Figure 3 shows that our dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward\n",
    "application of logistic regression will not perform well on this dataset\n",
    "since logistic regression will only be able to find a linear decision boundary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Feature mapping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to create more features\n",
    "def mapfeature(x1, x2, power):\n",
    "    '''\n",
    "    This function is to generate more features for training data. \n",
    "    How we generate more features: we will generate (power + 1)*(power + 2) / 2 features including 1, x1, x2, x1**2, x1*x2, x2**2, x1**3, x1*x2**2...x1*x2**(power-1), x2**power\n",
    "   \n",
    "    Input: \n",
    "         x1(ndarray): the data of one feature we currently have\n",
    "         x2(ndarray): the data of another feature we currently have\n",
    "         power(int): the highest power value that we want for our new features.\n",
    "\n",
    "    Output:\n",
    "         data_fm(2-D array): all the data (including original features and new ones)\n",
    "    '''\n",
    "    m = x1.shape[0]\n",
    "    n = int((power + 1)*(power + 2) / 2)\n",
    "    data_fm = np.ones((m, n))\n",
    "    col = 0\n",
    "    for i in range(0, power+1):\n",
    "        for j in range(0, i+1):\n",
    "            data_fm[:, col] = np.power(x1, j)*np.power(x2, i-j)\n",
    "            col += 1\n",
    "    return data_fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x_1 = microchip['Test1']\n",
    "x_2 = microchip['Test2']\n",
    "power_n = 6\n",
    "X_1 = mapfeature(x_1, x_2, power_n)"
   ]
  },
  {
   "source": [
    "## 4. Cost function and gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegCostFunction(theta, m, X, y, lam):\n",
    "    '''\n",
    "    This function is to calculate the cost of a logistic regression model.\n",
    "\n",
    "    Input:\n",
    "         theta(vector with shape (3, )): parameters\n",
    "         m(int,the number of rows in X or y, =X.shape[0]): the number of data groups\n",
    "         X(2-D array): training data\n",
    "         y(2-D array): flags\n",
    "         lam(int): the regularization parameter \n",
    "    Output:\n",
    "         cost(float): the cost of a logistic regression model\n",
    "    '''\n",
    "    theta = theta.reshape(1, 28)\n",
    "    h = sigmoid(X.dot(theta.T))\n",
    "    inner = - y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))\n",
    "    inner2 = np.power(theta, 2)\n",
    "    cost = (np.sum(inner) / m) + (lam*np.sum(inner2) / (2*m))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "m_1 = microchip.shape[0]\n",
    "theta_1 = np.array([0]*28)\n",
    "y_1 = microchip['Accepted']\n",
    "lam_1 = 0\n",
    "RegCostFunction(m=m_1, theta=theta_1, X=X_1, y=y_1, lam=lam_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegCostFunction(m=m_1, theta=theta_1, X=X_1, y=y_1, lam=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to perform gradient descent\n",
    "def gradient(theta, m, X, y, lam):\n",
    "    '''\n",
    "    This function is to calculate the gradient of cost function.\n",
    "\n",
    "    Input:\n",
    "         theta(vector with shape (3, )): parameters\n",
    "         m(int,the number of rows in X or y, =X.shape[0]): the number of data groups\n",
    "         X(2-D array): training data\n",
    "         y(2-D array): flags\n",
    "         lam(int): the regularization parameter \n",
    "    Output:\n",
    "         gd.flatten()(ndarray, vector with shape (3, )): gradients of each variable\n",
    "    '''\n",
    "    gd = np.ones((28, ))\n",
    "    X_0 = X[:, 0].reshape(118, 1)\n",
    "    diff = sigmoid(X.dot(theta.T)) - y\n",
    "    gd[0] = np.dot(X_0.T, diff) / m\n",
    "    X_1 = X[:, 1:]\n",
    "    gd[1:] = np.dot(X_1.T, diff) / m + lam*theta[1:] / m\n",
    "    return gd.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "gradient(theta=theta_1, m=m_1, X=X_1, y=y_1, lam=lam_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient(theta=theta_1, m=m_1, X=X_1, y=y_1, lam=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sop.minimize(fun=RegCostFunction, x0=theta_1, args=(m_1, X_1, y_1, lam_1), method='TNC', jac=gradient)\n",
    "theta_op = result.x"
   ]
  },
  {
   "source": [
    "## 2.5 Plotting the decision boundary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x1, x2):\n",
    "    '''\n",
    "    This function is to predict the probability of being accepted.\n",
    "    '''\n",
    "    power_0 = 6\n",
    "    X = mapfeature(x1, x2, 6)\n",
    "    probability = sigmoid(X.dot(theta_op.T))\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "x1 = microchip['Test1']\n",
    "x2 = microchip['Test2']\n",
    "model(x1, x2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(x1, x2, lam):\n",
    "    '''\n",
    "    This function is to predict whether being accepted.\n",
    "\n",
    "    Input:\n",
    "         x1(ndarray): the data of the first feature \n",
    "         x2(ndarray): the data of the second feature\n",
    "         lam(int): the regularization parameter\n",
    "    Output:\n",
    "         prediction_data(ndarray): the prediction results\n",
    "    '''\n",
    "    power_0 = 6\n",
    "    X = mapfeature(x1, x2, 6)\n",
    "    m_1 = microchip.shape[0]\n",
    "    theta_1 = np.array([0]*28)\n",
    "    y_1 = microchip['Accepted']\n",
    "    lam_1 = lam\n",
    "    result = sop.minimize(fun=RegCostFunction, x0=theta_1, args=(m_1, X_1, y_1, lam_1), method='TNC', jac=gradient)\n",
    "    theta_op = result.x\n",
    "    prediction_data = sigmoid(X.dot(theta_op.T))\n",
    "    prediction_data[prediction_data >= 0.5] = 1\n",
    "    prediction_data[prediction_data < 0.5] = 0\n",
    "    return prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "prediction(x1, x2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(x1, x2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_DB(lam):\n",
    "    '''\n",
    "    This function is to generate data for plotting the decision boundary.\n",
    "\n",
    "    Input:\n",
    "         lam(int): the regularization parameter\n",
    "    Output:\n",
    "         xx, yy(2-D array): the lines and rows of the grid, based on x and y values defined by np.arange(-1, 1.5, 0.005), the range is equal to the range of two original features.\n",
    "         zz(2-D array): the lines and rows of the grid, based on the predictions calculated with x, y values defined above.\n",
    "    '''\n",
    "    # Define the x and y scale\n",
    "    x1grid = np.arange(-1, 1.5, 0.005)\n",
    "    x2grid = np.arange(-1, 1.5, 0.005)\n",
    "    # Create all of the lines and rows of the grid\n",
    "    xx, yy = np.meshgrid(x1grid, x2grid)\n",
    "    # Flatten each grid to a vector\n",
    "    r1, r2 = xx.flatten(), yy.flatten()\n",
    "    r1, r2 = r1.reshape((len(r1), )), r2.reshape((len(r2), ))\n",
    "    # Make predictions for the grid\n",
    "    zgrid = prediction(r1, r2, lam)\n",
    "    # Reshape the predictions back into a grid\n",
    "    zz = zgrid.reshape(xx.shape)\n",
    "    return xx, yy, zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grid of x, y and z values as a surface\n",
    "acc = microchip.loc[microchip['Accepted'] == 1]\n",
    "rej = microchip.loc[microchip['Accepted'] == 0]\n",
    "λ = [0, 1, 10, 100]\n",
    "fig, ax = plt.subplots(4, figsize=(10, 40))\n",
    "for i in range(4):\n",
    "    data_pre = data_DB(λ[i])\n",
    "    ax[i].scatter(x=acc['Test1'], y=acc['Test2'], c='dodgerblue', marker='+', label='Accepted')\n",
    "    ax[i].scatter(x=rej['Test1'], y=rej['Test2'], c='r', marker='o', label='Rejected')\n",
    "    ax[i].contour(data_pre[0], data_pre[1], data_pre[2], 1, colors='b')\n",
    "    ax[i].set_xlabel('Microchip Test 1')\n",
    "    ax[i].set_ylabel('Microchip Test 2')\n",
    "    ax[i].set_title('Training data with decision boundary λ='+str(λ[i]))\n",
    "    ax[i].legend()"
   ]
  },
  {
   "source": [
    "Reference: \n",
    "- https://hackernoon.com/how-to-plot-a-decision-boundary-for-machine-learning-algorithms-in-python-3o1n3w07\n",
    "- https://blog.csdn.net/Mr_Cat123/article/details/80677525?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.control"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}